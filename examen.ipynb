{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorX = torch.tensor([1, 2, 3, 4])\n",
    "print(torch.unsqueeze(tensorX, 1))\n",
    "tensorX = torch.tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(tensorX, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000, 3.5000]), tensor([2., 3.]), tensor(2.5000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])  # invertir el resultado\n",
    "torch.mean(new_tensor, dim=1), torch.mean(new_tensor, dim=0), torch.mean(new_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clasificador de imágenes \n",
    "cat | cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "#### El Descenso de gradiente estocástico (SGD) \n",
    "***\n",
    "- Es un enfoque simple pero muy eficiente para el aprendizaje discriminativo de clasificadores lineales bajo funciones de pérdida convexa como Máquinas de vectores de soporte (lineales) y Regresión logística.\n",
    "- Tal vez sea más eficiente elegir ejemplos\n",
    "aleatorios.\n",
    "- Un solo ejemplo es muy poco, trabajamos con un batch.\n",
    "- calcula el gradiante usando una sola muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batch Stochastic Gradient Descent\n",
    "#### El descenso de gradiente de mini lotes \n",
    "***\n",
    "- Es una variación del algoritmo de descenso de gradiente que divide el conjunto de datos de entrenamiento en pequeños lotes que se utilizan para calcular el error del modelo y actualizar los coeficientes del modelo.\n",
    "- calcula el gradiante usando varias muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-Batch Gradient Descent\n",
    "- calcula el gradiante usando todo el conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Regresión | Clasificación |\n",
    "| -- | -- |\n",
    "| predecir cantidates | categorías |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised ML:\n",
    " - features\n",
    " - labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoide\n",
    "unos y ceros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss.backwark()\n",
    "- backpropagation\n",
    "  - que lo que estamos prediciendo tenga sentido\n",
    "  - la conexión en código que va a ir:\n",
    "    - primero hacia adelante\n",
    "    - luego, regresamos para arreglar algo\n",
    "    - volvemos a ir hacia adelante\n",
    "- train loop\n",
    "\n",
    "### optimizer.step()\n",
    "- mover el loop\n",
    "- vinculado a los gradiantes\n",
    "- loss funtion, de la perdida en base de la predicción y el dato real\n",
    "- luego repetimos con nuevos valores hasta que logremos un minimo\n",
    "### optimizer.zero_grad()\n",
    "reiniciar los gradiantes acumulados\n",
    "### Foward pass\n",
    "va a ver una predición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### función de activación en las neuronas de una NN\n",
    "- La función de activación que convierte las entradas ponderadas de una neurona a su activación a la salida.\n",
    "- La función de activación se encarga de devolver una salida a partir de un valor de entrada, normalmente el conjunto de valores de salida en un rango determinado como (0,1) o (-1,1).\n",
    "- Cada neurona tiene una función de activación y nos va a permitir conectar las múltiples capas para realizar la transformación de lineal a red neuronal\n",
    "- NN nos permite solucionar problemas no-lineales\n",
    "***\n",
    "- introduce no linealidad al input y el output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "- Cargar datos en un tamaño\n",
    "- Cada **bath** depende el tamaño dado a iterar\n",
    "- Carga una valuidación de datos\n",
    "- Proporciona una iterativo sobre el conjunto de datos | esta es la respuesta\n",
    "- **shuffle**, reorganiza los datos en cada epoch su es True\n",
    "***\n",
    "- En el clasificador de imágenes se implementa para dar un efectivo *Accuracy* en la validación de los datos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
